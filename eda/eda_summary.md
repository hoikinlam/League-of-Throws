For my final project, I'm planning on doing a classifier on League of Legends. Instead of predicting whether a team will win or lose, I decided to try to predict whether a team that has the upper hand will throw their lead and lose the game. The datasets I chose contains match information across the three highest ranks in the game (master, grandmaster, and challenger). The data contains both blue and red team information during their match that can influence the outcome. Some of the data includes: objectives (dragons and barons) which when slain can provide players with buffs. Other team related data includes: kills, gold, damage, towers, and etc which all factor into a team's performance. The key data is the outcome of a match, whether red team or blue team won the game. 

I chose this dataset because it provides a rich set of competitive features that would support my classification goals. Since league is a more strategic game, early advantages does not necessarily correlate to a guarantee victory as there are various factors that could impact the outcome such as, poor map control, bad teamfights, or minor mistakes. The dataset provides detailed match features along with events that indicate a team's first objective is ideal for modeling a relationship between early/mid/late game performances. Using data from the higher ranks ensure stability and consistency as higher ranked players tend to take the game more seriously thus, reducing the randomness and noise that appear in lower ranked games. It also help identify how frequently throws occur between different ranks. Overall,the dataset contains ~200,000 match details, 50+ feature columns, rank labels, throw outcomes (target), and win outcomes. Thus, I believe this dataset provides enough features to train a meaningful classifier. 

Some of the key variables I believe would be useful are the first objective ones such as first dragon and first tower, along with the gold and kills. I also calculated multiple differences between the two teams such as gold diff, kills diff, and objective diffs. These would help quantify team advantages to understand the magnitude of a lead. The dataset also contained no missing values so there is no need for imputations. 

From my exploratory data analysis, the analysis showed that throws most often occur when a team holds a moderate but not overwhelming lead, particularly in gold, kills, and objective control, while extremely large advantages almost never result in a loss. I also found that mid-games(~25:00 minutes) are far more prone to throws, suggesting that during the mid-game, matches increase volatility and the likelihood of losing control which does seem reasonable due to more player movement across the maps, more fights, and more aggressive plays. The reduced correlation analysis revealed that engineered “advantage metrics” such as gold_diff, kills_diff, and dragons_diff are far more informative than raw team stats and strongly relate to throw outcomes. I also found it interesting that the throw vs non-throw data is consistent across the various ranks. I was under the assumption that the higher ranks would be less-likely to throw due to the players being more competitive and knowledgeable/experienced in the game. There is no biases in terms of ranks or teams which indicates that the throw is more influenced by the objectives and numerical stats in the game. The heatmap is also useful in identifying correlations between each features, for example, the number of kills has a high positive correlation to the gold difference. 

Given what I observed during the EDA, several engineered features appear promising for improving model performance. First, the difference metrics—such as gold difference, kills difference, dragons difference, barons difference, and towers difference—captured much more predictive signal than the raw team stats. These directly quantify the magnitude and direction of a team’s lead, which aligns closely with the concept of a “throw.” Additionally, binary advantage indicators such as first blood, first tower, first dragon, and first baron can serve as early-game momentum features that help models understand whether a team established an early lead but ultimately lost. I may also explore normalized features, such as objectives per minute or gold per minute, to incorporate pacing into the model, since game duration showed a meaningful relationship with throw likelihood. Finally, categorical rank information could be included as a feature, though the EDA suggests that rank differences do not strongly influence throw behavior.

A major challenge revealed by the EDA is the strong class imbalance: only about 10% of all matches are throws. This imbalance could cause a naive classifier to predict “non-throw” for nearly every game while still achieving high accuracy. To mitigate this, I will need techniques such as resampling, class weighting, or choosing models that handle imbalance well (e.g., tree-based models). Another challenge is the high correlation among many features, especially within the raw team stats. Redundant variables may inflate model complexity and lead to overfitting, so feature selection or dimensionality reduction may be necessary. Additionally, the dataset only contains end-of-game or accumulated stats, so it lacks temporal information about when advantages were gained or lost. This makes it difficult to distinguish early-game leads from late-game momentum swings, which could be important for accurately predicting throws.

Lastly, there remain a few open questions that could influence feature engineering and modeling. The timing of objectives (early vs. late dragons or barons) may influence throw likelihood but is not directly observable from the available data. It is also uncertain whether rank should remain a feature, since throw behavior appeared consistent across Master, GrandMaster, and Challenger. Finally, potential interactions—such as baron control combined with gold lead—may be important for predicting throws and will need to be explored during modeling. Despite these limitations, the dataset provides strong, interpretable signals that can be leveraged to build a meaningful classifier.